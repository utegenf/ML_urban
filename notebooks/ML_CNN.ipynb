{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import IPython.display\nimport librosa\nimport librosa.display\nimport pandas as pd\nimport os\nimport struct\nimport glob\nimport soundfile as sf\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint \nfrom datetime import datetime\nfrom sklearn import metrics \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T16:08:03.960938Z","iopub.execute_input":"2022-02-20T16:08:03.961335Z","iopub.status.idle":"2022-02-20T16:08:03.970031Z","shell.execute_reply.started":"2022-02-20T16:08:03.961305Z","shell.execute_reply":"2022-02-20T16:08:03.969224Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Audio files and CSV file containing metadata\nfile_path = '../input/urbansound8k'\nurbansound8k = pd.read_csv('../input/urbansound8k/UrbanSound8K.csv')\nurbansound8k.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:08:03.971851Z","iopub.execute_input":"2022-02-20T16:08:03.972284Z","iopub.status.idle":"2022-02-20T16:08:04.013424Z","shell.execute_reply.started":"2022-02-20T16:08:03.972245Z","shell.execute_reply":"2022-02-20T16:08:04.012719Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class WavFileHelper():\n    \n    def read_file_properties(self, filename):\n\n        wave_file = open(filename,\"rb\")\n        \n        riff = wave_file.read(12)\n        fmt = wave_file.read(36)\n        \n        num_channels_string = fmt[10:12]\n        num_channels = struct.unpack('<H', num_channels_string)[0]\n\n        sample_rate_string = fmt[12:16]\n        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n        \n        bit_depth_string = fmt[22:24]\n        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n\n        return (num_channels, sample_rate, bit_depth)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:08:04.014552Z","iopub.execute_input":"2022-02-20T16:08:04.014821Z","iopub.status.idle":"2022-02-20T16:08:04.021536Z","shell.execute_reply.started":"2022-02-20T16:08:04.014791Z","shell.execute_reply":"2022-02-20T16:08:04.020642Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"wavfilehelper = WavFileHelper()\n\naudiodata = []\nfor index, row in urbansound8k.iterrows():\n    \n    file_name = os.path.join(os.path.abspath(file_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n    data = wavfilehelper.read_file_properties(file_name)\n    audiodata.append(data)\n\n# Convert into a Panda dataframe\naudiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:08:04.023224Z","iopub.execute_input":"2022-02-20T16:08:04.023439Z","iopub.status.idle":"2022-02-20T16:09:08.433000Z","shell.execute_reply.started":"2022-02-20T16:08:04.023414Z","shell.execute_reply":"2022-02-20T16:09:08.431965Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"max_pad_len = 174\n\ndef extract_features(file_name):\n   \n    try:\n        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n        pad_width = max_pad_len - mfccs.shape[1]\n        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n        \n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file_name)\n        return None \n     \n    return mfccs","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:09:08.434563Z","iopub.execute_input":"2022-02-20T16:09:08.434898Z","iopub.status.idle":"2022-02-20T16:09:08.441039Z","shell.execute_reply.started":"2022-02-20T16:09:08.434860Z","shell.execute_reply":"2022-02-20T16:09:08.440459Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"features = []\n\n# Iterate through each sound file and extract the features \nfor index, row in urbansound8k.iterrows():\n    \n    file_name = os.path.join(os.path.abspath(file_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n    \n    class_label = row[\"classID\"]\n    data = extract_features(file_name)\n    \n    features.append([data, class_label])\n\n# Convert into a Panda dataframe \nfeaturesdf = pd.DataFrame(features, columns=['feature','class_label'])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:09:08.442077Z","iopub.execute_input":"2022-02-20T16:09:08.442685Z","iopub.status.idle":"2022-02-20T16:24:32.372388Z","shell.execute_reply.started":"2022-02-20T16:09:08.442643Z","shell.execute_reply":"2022-02-20T16:24:32.371217Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"X = np.array(featuresdf.feature.tolist())\ny = np.array(featuresdf.class_label.tolist())\n\n# Encode the classification labels\nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y)) \n\n# split the dataset \nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 3)\n\nx_train1 = x_train \nx_test1 = x_test\ny_train1 = y_train\ny_test1 = y_test","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:24:32.374386Z","iopub.execute_input":"2022-02-20T16:24:32.374846Z","iopub.status.idle":"2022-02-20T16:24:32.607949Z","shell.execute_reply.started":"2022-02-20T16:24:32.374799Z","shell.execute_reply":"2022-02-20T16:24:32.607146Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"num_rows = 40\nnum_columns = 174\nnum_channels = 1\n\nx_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\nx_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\nprint(x_train.shape)\n\nnum_labels = yy.shape[1]\nfilter_size = 3","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:24:32.609186Z","iopub.execute_input":"2022-02-20T16:24:32.609391Z","iopub.status.idle":"2022-02-20T16:24:32.616007Z","shell.execute_reply.started":"2022-02-20T16:24:32.609365Z","shell.execute_reply":"2022-02-20T16:24:32.615054Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Constructing model with RELu and SoftMax activation functions:\nmodel_relu = Sequential()\nmodel_relu.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\nmodel_relu.add(MaxPooling2D(pool_size=(2,2)))\nmodel_relu.add(Dropout(0.2))\n\nmodel_relu.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\nmodel_relu.add(MaxPooling2D(pool_size=(2,2)))\nmodel_relu.add(Dropout(0.2))\n\nmodel_relu.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\nmodel_relu.add(MaxPooling2D(pool_size=(2,2)))\nmodel_relu.add(Dropout(0.2))\n\nmodel_relu.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\nmodel_relu.add(MaxPooling2D(pool_size=(2,2)))\nmodel_relu.add(Dropout(0.2))\nmodel_relu.add(GlobalAveragePooling2D())\nmodel_relu.add(Flatten())\nmodel_relu.add(Dense(num_labels, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:24:32.617429Z","iopub.execute_input":"2022-02-20T16:24:32.617684Z","iopub.status.idle":"2022-02-20T16:24:32.982736Z","shell.execute_reply.started":"2022-02-20T16:24:32.617654Z","shell.execute_reply":"2022-02-20T16:24:32.981795Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model_relu.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\nmodel_relu.summary()\n\n# Calculate pre-training accuracy \nscore = model_relu.evaluate(x_test, y_test, verbose=1)\naccuracy = 100*score[1]\n\nprint(\"Pre-training accuracy: %.4f%%\" % accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:24:32.985693Z","iopub.execute_input":"2022-02-20T16:24:32.986311Z","iopub.status.idle":"2022-02-20T16:24:34.284514Z","shell.execute_reply.started":"2022-02-20T16:24:32.986264Z","shell.execute_reply":"2022-02-20T16:24:34.283680Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nnum_batch_size = 256\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n                               verbose=1, save_best_only=True)\nstart = datetime.now()\n\nhistory_relu = model_relu.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data = (x_test, y_test), callbacks=[checkpointer], verbose=1)\n\nduration = datetime.now() - start\nprint(\"Training time is: \", duration)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:24:34.285818Z","iopub.execute_input":"2022-02-20T16:24:34.287127Z","iopub.status.idle":"2022-02-20T16:37:21.185378Z","shell.execute_reply.started":"2022-02-20T16:24:34.287089Z","shell.execute_reply":"2022-02-20T16:37:21.184382Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model on the training and testing set\n\nscore = model_relu.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy is: \", score[1])\n\nscore = model_relu.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy is: \", score[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:37:21.186518Z","iopub.execute_input":"2022-02-20T16:37:21.186769Z","iopub.status.idle":"2022-02-20T16:37:23.459098Z","shell.execute_reply.started":"2022-02-20T16:37:21.186740Z","shell.execute_reply":"2022-02-20T16:37:23.458310Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Plotting Loss and Accuracy graph\n\nmetrics = history_relu.history\nplt.plot(history_relu.epoch, metrics['loss'], metrics['val_loss'])\nplt.legend(['train_loss', 'test_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.show()\n\nplt.plot(history_relu.history['accuracy'], label='train_accuracy')\nplt.plot(history_relu.history['val_accuracy'], label='test_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:53:43.692959Z","iopub.execute_input":"2022-02-20T16:53:43.693271Z","iopub.status.idle":"2022-02-20T16:53:44.929090Z","shell.execute_reply.started":"2022-02-20T16:53:43.693237Z","shell.execute_reply":"2022-02-20T16:53:44.928338Z"},"trusted":true},"execution_count":41,"outputs":[]}]}